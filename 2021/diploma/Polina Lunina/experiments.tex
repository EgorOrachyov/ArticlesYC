Для экспериментальных исследований были необходимы данные двух типов: последовательности РНК для подачи на вход синтаксическому анализатору и эталонные вторичные структуры для этих последовательностей --- и то, и другое было получено из популярной в исследовательских работах базы данных RNAstrand~\cite{andronescu2008rna}. Эта база представляет собой сборку тщательно отобранных и приведенных к единому формату данных сразу из нескольких надежных баз, содержащих цепочки РНК вместе с полученными методами лабораторного эксперимента или эволюционного анализа вторичными структурами. Из выгруженных данных были удалены дубликаты, образцы с неточностями в нуклеотидной цепи или же вторичной структуре --- таким образом была получена выборка из 800 последовательностей длин от 1 до 100, для которой были сгенерированы матрицы разбора и матрицы контактов, переведенные в черно-белые изображения.

Для оценки качества работы обученных на данных изображениях нейронных сетей были выбраны следующие метрики, посчитанные относительно попиксельной разницы между предсказанным и эталонным изображениями. Далее $TW$ (true white), $FW$ (false white) и $FB$ (false black) --- информация о том, сколько раз нейронная сеть приняла верное и сколько раз неверное решение по каждому пикселю (кроме диагональных) каждого изображения тестовой выборки.
\begin{itemize} 
    \item $Precision = \frac{TW}{TW + FW}$ (доля предсказанных контактов, которые действительно являются контактами в эталонном изображении).
    \item $Recall = \frac{TW}{TW + FB}$ (доля найденных нейронной сетью контактов среди всех искомых).
    \item $F1 = 2 * \frac{Precision * Recall}{Precision + Recall}$ (гармоническое среднее $Precision$ и $Recall$, используется как удобная объединяющая метрика).
\end{itemize}

При обучении нейросети была использована функция потерь, в основе построения которой лежит идея о максимизации метрики $F1$ с несколькими уточнениями. Во-первых, $F1$ дискретна, а функция ошибки должна быть дифференцируема вследствие вычисления на ней градиента. Во-вторых, передача среднего по выборке значения $1 - F1$ в качестве функции ошибки не гарантирует отсутствие большого разброса $Precision$ и $Recall$ как в пределах отдельно взятого изображения, так и в масштабах всей выборки, следствием чего будет нестабильность качества работы модели и высокая вероятность появления очень низкой точности результата для случайно взятого тестового образца. На основании данных соображений была реализована функция $F1\_loss$, представленная на рис.~\ref{loss}. Здесь дифференцируемость обеспечивается заменой сумм дискретных целочисленных значений на непрерывную сумму значений вероятности, а поддержка баланса между $Precision$ и $Recall$ для каждого изображения и для выборки в целом --- двумя пропорциональными величине разброса штрафными коэффициентами $k1$ и $k2$, накладываемыми на метрику $F1$.

\begin{figure}[h]
\begin{center}
\centering
\begin{python}
from keras import backend as K

def f1_loss(y_true, y_pred):
    #normalize pixels values to [0, 1]
    y_true, y_pred = K.minimum(y_true / 255, 1), K.minimum(y_pred / 255, 1)
    #calculate differentiable versions of TW, FW and FB
    tw = K.sum(K.cast(y_true * y_pred, 'float32'), axis=[1, 2, 3])
    fw = K.sum(K.cast((1 - y_true) * y_pred, 'float32'), axis=[1, 2, 3])
    fb = K.sum(K.cast(y_true * (1 - y_pred), 'float32'), axis=[1, 2, 3])
    #calculate precision and recall secure from zero division error
    precision = tw / (tw + fw + K.epsilon())
    recall = tw / (tw + fb + K.epsilon())
    #penalty coefficients for huge difference between precision and recall 
    #calculated for each image and whole dataset respectively
    k1 = 1 -  K.abs(precision - recall)
    k2 = 1 -  K.abs(K.mean(precision) - K.mean(recall))
    #calculate upgraded f1 score
    f1 = k1 * k2 * 2 * precision * recall / (precision + recall + K.epsilon()) 
    return 1 - K.mean(f1)
\end{python}
\caption{Функция потерь нейронной сети}
\label{loss}
\end{center}
\end{figure} 

Для сравнения результатов работы обученной модели с существующими в области аналогами был проведен анализ различных инструментов, предсказывающих вторичную структуру РНК, по следующим критериям: заявленная высокая точность результатов, возможность предсказания псевдоузлов, удобство использования и адекватное время работы. На основании данных соображений были отобраны пять инструментов, основанных на различных подходах.
\begin{itemize}
    \item HotKnots --- минимизации свободной энергии через эвристический алгоритм~\cite{ren2005hotknots}.
    \item SPOT-RNA --- глубокое обучение, основанное на технике transfer learning~\cite{singh2019rna}.
    \item PknotsRG --- минимизация свободной энергии с использованием Turner energy rules)~\cite{reeder2007pknotsrg}.
    \item RNAstructure --- минимизация свободной энергии с помощью динамического программирования~\cite{bellaousov2013rnastructure}.
    \item Ipknot --- максимизация оценки точности алгоритма методом целочисленного программирования~\cite{sato2011ipknot}.
\end{itemize}

\subsection{Результаты}
На рис.~\ref{plot_f1} представлены результаты тестирования обученной модели по метрике F1  для различных процентов разделения данных на тестовую и обучающую выборки, а также продемострированная пятью вышеописанными инструментами точность на всей исследуемой выборке. Практически любой неронной сети требуется большое количество данных для обучения, поэтому на малых размерах обучающей выборки наша модель демонстрирует более низкие результаты, чем конкуренты, однако уже на 40\% она догоняет, а после --- и обгоняет другие инструменты. 

На рис.~\ref{plot_pr} показаны результаты аналогичного тестирования моделей по метрикам $Precision$ и $Recall$; здесь черная прямая $y=x$ символизирует оптимальное положение этих метрик --- их равенство. На графике можно увидеть, что значения метрик для нашей модели расположены достаточно близко к желаемой прямой, что говорит о высоком качестве работы нейросети. Кроме того, реализованный в данной работе алгоритм --- единственный из приведенного сравнения, имеющий $Recall$, больший, чем $Precision$: это произошло из-за того, что парсер находит значительную часть требуемых контактов, и владеющая этой информацией еще до начала обучения нейронная сеть не нуждается в подборе закономерностей для их поиска, что делает наш подход несколько нетрадиционным относительно аналогов. 

\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \fbox{\includegraphics[width=.9\linewidth]{pics/plot_f1.png}}
  \caption{Значения метрики $F1$}
  \label{plot_f1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \fbox{\includegraphics[width=.9\linewidth]{pics/plot_pr.png}}
  \caption{Значения метрик $Precision$ и $Recall$}
  \label{plot_pr}
\end{subfigure}
\caption{Сравнение разработанного подхода с аналогами}
\label{plot}
\end{figure}

Таким образом, экспериментальные исследования показали работоспособность разработанного в данной работе подхода применительно к задаче предсказания вторичной структуры РНК даже в сравнении с лучшими инструментами в области.