\section{introduction}

One of the promising ways to high-performance graph analysis is based on utilization of linear algebra: operations over vectors and matrices can be efficiently implemented on modern parallel hardware, and once we reduce given graph analysis problem to composition of such operations, we get high-performance solution for our problem. 
Well-known example of such reduction is a reduction of all-pairs shortest path (APSP) problem to matrix multiplication over appropriate \textit{semiring}.
To formalize, generalize this observation and make it useful in practice, GraphBLAS API standard was found~\cite{7761646}. 
GraphBLAS API introduces appropriate algebraic structures (monoid, semiring), objects (scalar, vector, matrix), and operations over them such that to form building blocks to create graph analysis algorithms.  
Not only Graphs. Sparse Linear algebra. 

Reference implementation SuiteSarse:GraphBLAS~\cite{10.1145/3322125}.
Other implementations: CombBLAS, !!! .

GPGPU for high-performance analysis of huge amount of data. 
GraphBLAST~\cite{yang2019graphblast} --- GraphBLAST in CUDA C. 

High-level programming languages for application development vs low-level for high-performance programming.
Moreover, specific languages for GPGPU programming: CUDA C, OpenCL C.
Problems with types, compositionality, optimizations (kernel fusion).

Functional programming. Type systems. Optimizations. Futhark~\cite{Henriksen:2017:FPF:3062341.3062354}, kernel fusion, specialization, deforestation etc. 

In this work we discuss some !!!, provide !!! which we start working on. It is implemented on .NET platform in F\# programming language with OpenCL backend. 
Portability of OpenCL: CPU, GPGPU, FPGA~\cite{kenter2019invited,6567546}.
A way to hardware acceleration of sparse linear algebra and graph analysis.
Brahma.FSharp\footnote{!!!}.
Our preliminary evaluation shows that !!!1