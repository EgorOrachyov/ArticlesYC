\section{Introduction}

One of the techniques to efficiently solve a data analysis problem is to formulate it in terms of linear algebra (in terms of operations over vectors and matrices).
That gives one well studied for years mathematical tools and solutions, as well as the possibility to evaluate this problem with \textit{zero-cost} by high-performance linear algebra libraries, which utilize modern hardware, provide various optimization techniques, and allow quickly and safely prototype solution in code with predefined building blocks.
GraphBLAS API\footnote{GraphBLAS project web page: \url{https://graphblas.github.io/}. Access date: 19.01.2021.}~\cite{7761646} is one of the standards that introduce such building blocks.
GraphBLAS take into account sparsity of data by using sparse formats of matrices and vectors, and operates with arbitrary \textit{monoids} and \textit{semirings} to make provided building blocs generic.
While initially GraphBLAS was focused on graph analysis, it was shown that the proposed approach can be successfully used for data analysis in other areas, such as computational biology~\cite{10.5555/3433701.3433800} and machine learning~\cite{8091098}. 

GPGPU utilization for data analysis and for linear algebra operations is a promising way to high-performance data analysis because GPGPU gives much more power in parallel data processing.
But the implementation of appropriate libraries is very challenging.
GPGPU programming introduces heterogeneous device model into the system, memory traffic, and data operations limitations, as well as requires taking into account vendor-specific capabilities.
Thus, there is no, best to our knowledge, full implementation of GraphBLAS API on GPGPU, except GraphBLAST project\footnote{GraphBLAST project: \url{https://github.com/gunrock/graphblast}. Access date: 19.01.2021.}~\cite{yang2019graphblast}, which currently in active development.

The sparsity of data introduces problems with load balancing, irregular data access, thus sparsity makes the implementation of high-performance algorithms for sparse linear algebra on GPGPU even more challenging.
As a result, there is a huge number of different formats for sparse matrices and vectors representation, such as CSR, COO, Quad-tree, and a huge number of algorithms for operations over these formats. 
For example, one can look at the significant survey of sparse matrix-matrix multiplication algorithms~\cite{Gao2020ASS}. 
Unfortunately, algorithms for different operations, such as matrix-matrix multiplication, matrix-vector multiplication, etc. are developed independently. 
Thus, there are no sparse linear algebra libraries based on state-of-the-art algorithms.
Moreover, existing libraries, such as cuSparse\footnote{NVIDIA sparse matrix library (in Cuda) \url{https://docs.nvidia.com/cuda/cusparse/}. Access date: 19.01.2021.}, clSparse\footnote{Sparse linear library functions in OpenCL: \url{http://clmathlibraries.github.io/clSPARSE/}. Access date: 19.01.2021.}~\cite{10.1145/2909437.2909442}, or more modern CUSP\footnote{CUSP sparse linear algebra library: \url{https://cusplibrary.github.io/modules.html}. Access date: 19.01.2021.} or bhSparse\footnote{bhSparse sparse matrix multiplication library: \url{https://github.com/weifengliu-ssslab/bhSPARSE}. Access date: 19.01.2021.}~\cite{10.1016/j.jpdc.2015.06.010}, are focused on numerical computations over floats or doubles, not on generic data processing over arbitrary semirings which required for GraphBLAS API implementation.

An important partial case of linear algebra is as sparse Boolean linear algebra.
Boolean algebra allows to address problems over a finite set of values, for example, transitive closure of relation or graph, regular and context-free path queries for graphs~\cite{10.1145/3210259.3210264}, parsing for different classes of languages, such as Context-Free~\cite{10.1016/S0022-0000(75)80046-8}, Boolean and Conjunctive~\cite{OKHOTIN2014101}, Multiple Context-Free(MCFL)~\cite{10.5555/972525.972527}.
Moreover, some operations over Boolean semiring may be used as building blocks for algorithms over other semirings. 
For example, to compute the shape of the result of the operation.
Thus, sparse Boolean linear algebra is an important partial case both as a way to solve applied problems and as a building block for other algorithms.
However, sparse Boolean linear algebra on GPGPU is still not presented, because of its high specificity.

In this work, we present the sparse boolean linear algebra operations implementation as stand-alone self-sufficient programming libraries for the two most popular GPGPU platforms: NVIDIA Cuda\footnote{CUDA is a platform and programming model for NVIDIA devices. Home page: \url{https://developer.nvidia.com/CUDA-zone}. Access date: 19.01.2021.}and OpenCL\footnote{OpenCL is an open standard for parallel programming of heterogeneous systems. Home page: \url{https://www.khronos.org/opencl/}. Access date: 19.01.2021.}.
Cuda is a GPGPU technology for NVIDIA devices, which allows to employ of some platform-specific facilities, such as unified memory mechanism, and make architectural assumptions, which gives more optimizations space at cost of portability. 
OpenCL is a platform-agnostic API standard, which allows running computations on different platforms, such as multi-threaded CPUs, GPUs, and FPGAs.
Our implementation relies on modern sparse matrices processing techniques, as well as exploits some optimizations, related to the boolean data processing.
A few words on Python ALI and evaluation results!!!